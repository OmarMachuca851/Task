{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarMachuca851/Task/blob/main/sec2sec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bca6cc40",
      "metadata": {
        "id": "bca6cc40"
      },
      "source": [
        "# sec2sec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d936c8d6",
      "metadata": {
        "id": "d936c8d6"
      },
      "source": [
        "## Ejecución de traducción automática y lectura de código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99f39b5c",
      "metadata": {
        "id": "99f39b5c",
        "outputId": "cfc28ef9-5375-436e-934e-1098e9bbf767"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples: 10000\n",
            "Number of unique input tokens: 70\n",
            "Number of unique output tokens: 91\n",
            "Max sequence length for inputs: 14\n",
            "Max sequence length for outputs: 59\n",
            "WARNING:tensorflow:From /home/omar/workspace/miniconda3/envs/tf115/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/omar/workspace/miniconda3/envs/tf115/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/omar/workspace/miniconda3/envs/tf115/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/omar/workspace/miniconda3/envs/tf115/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "2025-09-28 21:23:24.250434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2025-09-28 21:23:24.331059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2025-09-28 21:23:24.335332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: NVIDIA GeForce RTX 2060 major: 7 minor: 5 memoryClockRate(GHz): 1.65\n",
            "pciBusID: 0000:01:00.0\n",
            "2025-09-28 21:23:24.335485: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2025-09-28 21:23:24.336200: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2025-09-28 21:23:24.336888: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2025-09-28 21:23:24.337070: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2025-09-28 21:23:24.337923: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2025-09-28 21:23:24.338558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2025-09-28 21:23:24.340426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2025-09-28 21:23:24.340509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2025-09-28 21:23:24.342329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2025-09-28 21:23:24.345894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2025-09-28 21:23:24.346522: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2025-09-28 21:23:24.349498: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3302400000 Hz\n",
            "2025-09-28 21:23:24.349724: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3d452740 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2025-09-28 21:23:24.349737: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2025-09-28 21:23:24.454787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2025-09-28 21:23:24.455865: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3ca1fef0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2025-09-28 21:23:24.455879: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2060, Compute Capability 7.5\n",
            "2025-09-28 21:23:24.456030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2025-09-28 21:23:24.457101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: NVIDIA GeForce RTX 2060 major: 7 minor: 5 memoryClockRate(GHz): 1.65\n",
            "pciBusID: 0000:01:00.0\n",
            "2025-09-28 21:23:24.457140: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2025-09-28 21:23:24.457151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2025-09-28 21:23:24.457160: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2025-09-28 21:23:24.457167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2025-09-28 21:23:24.457176: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2025-09-28 21:23:24.457184: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2025-09-28 21:23:24.457192: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2025-09-28 21:23:24.457239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2025-09-28 21:23:24.458158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2025-09-28 21:23:24.459035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2025-09-28 21:23:24.459061: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2025-09-28 21:23:24.460032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2025-09-28 21:23:24.460040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2025-09-28 21:23:24.460043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2025-09-28 21:23:24.460103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2025-09-28 21:23:24.461005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2025-09-28 21:23:24.462200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10799 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/omar/workspace/miniconda3/envs/tf115/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/omar/workspace/miniconda3/envs/tf115/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/100\n",
            "  64/8000 [..............................] - ETA: 49s - loss: 4.5092 - accuracy: 0.0045"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-28 21:23:25.153221: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8000/8000 [==============================] - 3s 392us/step - loss: 1.1304 - accuracy: 0.7385 - val_loss: 1.0124 - val_accuracy: 0.7209\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 3s 326us/step - loss: 0.8223 - accuracy: 0.7798 - val_loss: 0.8074 - val_accuracy: 0.7703\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 3s 324us/step - loss: 0.6515 - accuracy: 0.8169 - val_loss: 0.6833 - val_accuracy: 0.8031\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 3s 330us/step - loss: 0.5644 - accuracy: 0.8359 - val_loss: 0.6167 - val_accuracy: 0.8176\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 3s 318us/step - loss: 0.5151 - accuracy: 0.8494 - val_loss: 0.5830 - val_accuracy: 0.8276\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4791 - accuracy: 0.8601 - val_loss: 0.5489 - val_accuracy: 0.8395\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.4517 - accuracy: 0.8673 - val_loss: 0.5309 - val_accuracy: 0.8437\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.4293 - accuracy: 0.8729 - val_loss: 0.5125 - val_accuracy: 0.8489\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 3s 319us/step - loss: 0.4092 - accuracy: 0.8781 - val_loss: 0.4972 - val_accuracy: 0.8529\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 3s 319us/step - loss: 0.3915 - accuracy: 0.8830 - val_loss: 0.4840 - val_accuracy: 0.8571\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 3s 318us/step - loss: 0.3749 - accuracy: 0.8879 - val_loss: 0.4739 - val_accuracy: 0.8599\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 3s 319us/step - loss: 0.3595 - accuracy: 0.8923 - val_loss: 0.4663 - val_accuracy: 0.8611\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 2s 308us/step - loss: 0.3452 - accuracy: 0.8963 - val_loss: 0.4577 - val_accuracy: 0.8649\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.3311 - accuracy: 0.9004 - val_loss: 0.4534 - val_accuracy: 0.8659\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 2s 307us/step - loss: 0.3184 - accuracy: 0.9042 - val_loss: 0.4464 - val_accuracy: 0.8685\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 2s 306us/step - loss: 0.3063 - accuracy: 0.9077 - val_loss: 0.4426 - val_accuracy: 0.8702\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.2948 - accuracy: 0.9113 - val_loss: 0.4386 - val_accuracy: 0.8718\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.2839 - accuracy: 0.9144 - val_loss: 0.4384 - val_accuracy: 0.8718\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 3s 318us/step - loss: 0.2733 - accuracy: 0.9176 - val_loss: 0.4360 - val_accuracy: 0.8724\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 3s 328us/step - loss: 0.2638 - accuracy: 0.9206 - val_loss: 0.4337 - val_accuracy: 0.8743\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 3s 344us/step - loss: 0.2540 - accuracy: 0.9232 - val_loss: 0.4350 - val_accuracy: 0.8755\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 3s 323us/step - loss: 0.2451 - accuracy: 0.9259 - val_loss: 0.4363 - val_accuracy: 0.8759\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.2363 - accuracy: 0.9281 - val_loss: 0.4382 - val_accuracy: 0.8754\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.2282 - accuracy: 0.9307 - val_loss: 0.4386 - val_accuracy: 0.8758\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.2203 - accuracy: 0.9332 - val_loss: 0.4357 - val_accuracy: 0.8771\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.2130 - accuracy: 0.9353 - val_loss: 0.4384 - val_accuracy: 0.8779\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.2056 - accuracy: 0.9377 - val_loss: 0.4418 - val_accuracy: 0.8780\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.1990 - accuracy: 0.9397 - val_loss: 0.4469 - val_accuracy: 0.8767\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.1925 - accuracy: 0.9413 - val_loss: 0.4483 - val_accuracy: 0.8783\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.1866 - accuracy: 0.9431 - val_loss: 0.4506 - val_accuracy: 0.8781\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.1807 - accuracy: 0.9449 - val_loss: 0.4517 - val_accuracy: 0.8785\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.1748 - accuracy: 0.9467 - val_loss: 0.4625 - val_accuracy: 0.8763\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 2s 311us/step - loss: 0.1696 - accuracy: 0.9483 - val_loss: 0.4615 - val_accuracy: 0.8778\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 3s 319us/step - loss: 0.1646 - accuracy: 0.9497 - val_loss: 0.4665 - val_accuracy: 0.8778\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.1596 - accuracy: 0.9511 - val_loss: 0.4700 - val_accuracy: 0.8781\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.1554 - accuracy: 0.9523 - val_loss: 0.4732 - val_accuracy: 0.8782\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 3s 348us/step - loss: 0.1506 - accuracy: 0.9538 - val_loss: 0.4746 - val_accuracy: 0.8782\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 3s 342us/step - loss: 0.1463 - accuracy: 0.9550 - val_loss: 0.4819 - val_accuracy: 0.8782\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 3s 341us/step - loss: 0.1420 - accuracy: 0.9562 - val_loss: 0.4871 - val_accuracy: 0.8775\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 3s 343us/step - loss: 0.1382 - accuracy: 0.9576 - val_loss: 0.4897 - val_accuracy: 0.8786\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 3s 341us/step - loss: 0.1345 - accuracy: 0.9585 - val_loss: 0.4945 - val_accuracy: 0.8780\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 3s 340us/step - loss: 0.1308 - accuracy: 0.9594 - val_loss: 0.4983 - val_accuracy: 0.8779\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 3s 341us/step - loss: 0.1274 - accuracy: 0.9606 - val_loss: 0.5018 - val_accuracy: 0.8781\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 3s 337us/step - loss: 0.1241 - accuracy: 0.9614 - val_loss: 0.5048 - val_accuracy: 0.8789\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 3s 335us/step - loss: 0.1207 - accuracy: 0.9627 - val_loss: 0.5082 - val_accuracy: 0.8785\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 3s 337us/step - loss: 0.1176 - accuracy: 0.9633 - val_loss: 0.5173 - val_accuracy: 0.8770\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 3s 334us/step - loss: 0.1147 - accuracy: 0.9644 - val_loss: 0.5220 - val_accuracy: 0.8780\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 3s 336us/step - loss: 0.1118 - accuracy: 0.9652 - val_loss: 0.5229 - val_accuracy: 0.8777\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 3s 335us/step - loss: 0.1093 - accuracy: 0.9660 - val_loss: 0.5312 - val_accuracy: 0.8763\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 3s 332us/step - loss: 0.1065 - accuracy: 0.9667 - val_loss: 0.5384 - val_accuracy: 0.8767\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 3s 335us/step - loss: 0.1040 - accuracy: 0.9674 - val_loss: 0.5404 - val_accuracy: 0.8772\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 3s 348us/step - loss: 0.1017 - accuracy: 0.9680 - val_loss: 0.5430 - val_accuracy: 0.8769\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 3s 330us/step - loss: 0.0992 - accuracy: 0.9688 - val_loss: 0.5463 - val_accuracy: 0.8768\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 3s 320us/step - loss: 0.0970 - accuracy: 0.9694 - val_loss: 0.5593 - val_accuracy: 0.8767\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 0.0951 - accuracy: 0.9699 - val_loss: 0.5578 - val_accuracy: 0.8769\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 3s 323us/step - loss: 0.0925 - accuracy: 0.9708 - val_loss: 0.5599 - val_accuracy: 0.8771\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 3s 327us/step - loss: 0.0907 - accuracy: 0.9711 - val_loss: 0.5679 - val_accuracy: 0.8769\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 3s 324us/step - loss: 0.0885 - accuracy: 0.9719 - val_loss: 0.5752 - val_accuracy: 0.8764\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 0.0871 - accuracy: 0.9723 - val_loss: 0.5739 - val_accuracy: 0.8764\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.0850 - accuracy: 0.9727 - val_loss: 0.5818 - val_accuracy: 0.8767\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.0831 - accuracy: 0.9733 - val_loss: 0.5800 - val_accuracy: 0.8763\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 0.0815 - accuracy: 0.9739 - val_loss: 0.5918 - val_accuracy: 0.8767\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.0798 - accuracy: 0.9744 - val_loss: 0.5923 - val_accuracy: 0.8764\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 0.0781 - accuracy: 0.9747 - val_loss: 0.5968 - val_accuracy: 0.8762\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.0767 - accuracy: 0.9751 - val_loss: 0.6009 - val_accuracy: 0.8763\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 0.0752 - accuracy: 0.9756 - val_loss: 0.6067 - val_accuracy: 0.8761\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.0734 - accuracy: 0.9760 - val_loss: 0.6097 - val_accuracy: 0.8762\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.0720 - accuracy: 0.9764 - val_loss: 0.6128 - val_accuracy: 0.8758\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.0708 - accuracy: 0.9768 - val_loss: 0.6150 - val_accuracy: 0.8752\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.0693 - accuracy: 0.9772 - val_loss: 0.6201 - val_accuracy: 0.8763\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.0677 - accuracy: 0.9777 - val_loss: 0.6233 - val_accuracy: 0.8759\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 2s 312us/step - loss: 0.0670 - accuracy: 0.9779 - val_loss: 0.6270 - val_accuracy: 0.8762\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 3s 313us/step - loss: 0.0660 - accuracy: 0.9781 - val_loss: 0.6309 - val_accuracy: 0.8758\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 3s 319us/step - loss: 0.0647 - accuracy: 0.9784 - val_loss: 0.6335 - val_accuracy: 0.8759\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.0636 - accuracy: 0.9789 - val_loss: 0.6347 - val_accuracy: 0.8762\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 0.0625 - accuracy: 0.9791 - val_loss: 0.6429 - val_accuracy: 0.8755\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.0615 - accuracy: 0.9793 - val_loss: 0.6431 - val_accuracy: 0.8759\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 3s 314us/step - loss: 0.0601 - accuracy: 0.9795 - val_loss: 0.6454 - val_accuracy: 0.8763\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 3s 315us/step - loss: 0.0593 - accuracy: 0.9800 - val_loss: 0.6510 - val_accuracy: 0.8741\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 3s 317us/step - loss: 0.0585 - accuracy: 0.9800 - val_loss: 0.6551 - val_accuracy: 0.8757\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 3s 328us/step - loss: 0.0573 - accuracy: 0.9806 - val_loss: 0.6604 - val_accuracy: 0.8753\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 3s 343us/step - loss: 0.0565 - accuracy: 0.9808 - val_loss: 0.6608 - val_accuracy: 0.8757\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 3s 329us/step - loss: 0.0557 - accuracy: 0.9810 - val_loss: 0.6648 - val_accuracy: 0.8758\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 3s 328us/step - loss: 0.0550 - accuracy: 0.9810 - val_loss: 0.6689 - val_accuracy: 0.8748\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 3s 321us/step - loss: 0.0543 - accuracy: 0.9811 - val_loss: 0.6731 - val_accuracy: 0.8745\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 3s 326us/step - loss: 0.0532 - accuracy: 0.9815 - val_loss: 0.6750 - val_accuracy: 0.8752\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 3s 323us/step - loss: 0.0524 - accuracy: 0.9817 - val_loss: 0.6747 - val_accuracy: 0.8753\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 3s 321us/step - loss: 0.0517 - accuracy: 0.9820 - val_loss: 0.6833 - val_accuracy: 0.8753\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 3s 325us/step - loss: 0.0513 - accuracy: 0.9823 - val_loss: 0.6861 - val_accuracy: 0.8758\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 3s 319us/step - loss: 0.0501 - accuracy: 0.9826 - val_loss: 0.6865 - val_accuracy: 0.8754\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 3s 317us/step - loss: 0.0498 - accuracy: 0.9826 - val_loss: 0.6878 - val_accuracy: 0.8750\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 3s 319us/step - loss: 0.0491 - accuracy: 0.9828 - val_loss: 0.6940 - val_accuracy: 0.8758\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 3s 321us/step - loss: 0.0484 - accuracy: 0.9829 - val_loss: 0.6962 - val_accuracy: 0.8749\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 3s 317us/step - loss: 0.0479 - accuracy: 0.9831 - val_loss: 0.6960 - val_accuracy: 0.8758\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 3s 319us/step - loss: 0.0472 - accuracy: 0.9832 - val_loss: 0.6988 - val_accuracy: 0.8747\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 3s 320us/step - loss: 0.0469 - accuracy: 0.9835 - val_loss: 0.7044 - val_accuracy: 0.8747\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 3s 316us/step - loss: 0.0461 - accuracy: 0.9836 - val_loss: 0.7079 - val_accuracy: 0.8754\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 3s 320us/step - loss: 0.0455 - accuracy: 0.9838 - val_loss: 0.7113 - val_accuracy: 0.8751\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 3s 320us/step - loss: 0.0453 - accuracy: 0.9839 - val_loss: 0.7063 - val_accuracy: 0.8752\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 3s 324us/step - loss: 0.0445 - accuracy: 0.9841 - val_loss: 0.7182 - val_accuracy: 0.8746\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Marche.\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Marche.\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Marche.\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Marche.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Salut.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Salut.\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: File !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: File !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: File !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: File !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: File !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: File !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: File !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: File !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: File !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: File !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: File !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: File !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: File !\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: File !\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "\"\"\"\n",
        "## Download the data\n",
        "\"\"\"\n",
        "\n",
        "# fpath = keras.utils.get_file(origin=\"http://www.manythings.org/anki/fra-eng.zip\")\n",
        "# dirpath = Path(fpath).parent.absolute()\n",
        "# os.system(f\"unzip -q {fpath} -d {dirpath}\")\n",
        "\n",
        "\"\"\"\n",
        "## Configuration\n",
        "\"\"\"\n",
        "\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = os.path.join(\"./data/fra-eng\", \"fra.txt\")\n",
        "\n",
        "\"\"\"\n",
        "## Prepare the data\n",
        "\"\"\"\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "\"\"\"\n",
        "## Build the model\n",
        "\"\"\"\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "\"\"\"\n",
        "## Train the model\n",
        "\"\"\"\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "# Save model\n",
        "model.save(\"s2s_model.keras\")\n",
        "\n",
        "\"\"\"\n",
        "## Run inference (sampling)\n",
        "\n",
        "1. encode input and retrieve initial decoder state\n",
        "2. run one step of decoder with this initial state\n",
        "and a \"start of sequence\" token as target.\n",
        "Output will be the next target token.\n",
        "3. Repeat with the current target token and current states\n",
        "\"\"\"\n",
        "\n",
        "# Define sampling models\n",
        "# Restore the model and construct the encoder and decoder.\n",
        "model = keras.models.load_model(\"s2s_model.keras\")\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value, verbose=0\n",
        "        )\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "You can now generate decoded sentences as such:\n",
        "\"\"\"\n",
        "\n",
        "for seq_index in range(20):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", input_texts[seq_index])\n",
        "    print(\"Decoded sentence:\", decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c165ab15",
      "metadata": {
        "id": "c165ab15"
      },
      "source": [
        "### Lectura del código:\n",
        "en mi caso utiliza datos locales (fra.txt)\n",
        "- linea 18-23: configuración de variables globales\n",
        "- linea 30-91: preparación de los datos (decodificación y tokenización)\n",
        "- linea 98-118: construcción del modelo\n",
        "- linea 124-133: entrenamiento del modelo\n",
        "- linea 135: guardado del modelo\n",
        "- linea 139: cargado del modelo\n",
        "- linea 151-169: obtención de los datos códificados y decodificación de los datos\n",
        "- linea 173-174: detokenización (reversión de codificación a caracteres para ser posible una lectura)\n",
        "- linea 177-211: funcion de decodificación de secuencia\n",
        "- linea 218-225: parte del código que permite comprobar el funcionamiento de las predicciones"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81132142",
      "metadata": {
        "id": "81132142"
      },
      "source": [
        "## Ejecución de un modelo preentrenado para subtítulos de imágenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36e374ba",
      "metadata": {
        "id": "36e374ba",
        "outputId": "882d49c5-7b9d-4761-cad7-98a5c150362c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start> a group of giraffes standing next to each other . <end>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\OMC\\Documents\\DProClass\\python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\OMC\\Documents\\DProClass\\python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "!python pytorchtutorial/tutorials/03-advanced/image_captioning/sample.py --image=\"pytorchtutorial/tutorials/03-advanced/image_captioning/png/example.png\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f035fd53",
      "metadata": {
        "id": "f035fd53"
      },
      "source": [
        "## Investigar qué hacer si quieres ejecutarlo con Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e6c879f",
      "metadata": {
        "id": "4e6c879f"
      },
      "source": [
        "### Consideraciones importantes:\n",
        "- Compatibilidad de arquitecturas: Las arquitecturas deben ser equivalentes\n",
        "\n",
        "- Orden de dimensiones: PyTorch usa (C, H, W), Keras usa (H, W, C)\n",
        "\n",
        "- Nombres de capas: Pueden diferir entre frameworks\n",
        "\n",
        "- Operaciones específicas: Algunas operaciones pueden no ser directamente transferibles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fe471b3",
      "metadata": {
        "id": "4fe471b3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def load_pytorch_weights(keras_model, pytorch_weights_path):\n",
        "    \"\"\"\n",
        "    Carga pesos desde un modelo PyTorch .pkl a un modelo Keras\n",
        "    \"\"\"\n",
        "    # Cargar pesos de PyTorch\n",
        "    pytorch_weights = torch.load(pytorch_weights_path, map_location='cuda:0')\n",
        "\n",
        "    if 'state_dict' in pytorch_weights:\n",
        "        pytorch_weights = pytorch_weights['state_dict']\n",
        "\n",
        "    # Mapeo de nombres de capas\n",
        "    keras_weights = []\n",
        "\n",
        "    # Extraer pesos en el orden correcto para Keras\n",
        "    for name, param in pytorch_weights.items():\n",
        "        if 'resnet' in name and 'weight' in name and param.ndim == 4:\n",
        "            # Pesos convolucionales: (out_c, in_c, h, w) -> (h, w, in_c, out_c)\n",
        "            weight = param.numpy().transpose(2, 3, 1, 0)\n",
        "            keras_weights.append(weight)\n",
        "        elif 'linear.weight' in name:\n",
        "            # Capa densa: (out_features, in_features) -> (in_features, out_features)\n",
        "            weight = param.numpy().transpose(1, 0)\n",
        "            keras_weights.append(weight)\n",
        "        elif 'linear.bias' in name or 'bn' in name:\n",
        "            # Sesgos y parámetros de BatchNorm\n",
        "            keras_weights.append(param.numpy())\n",
        "\n",
        "    # Establecer pesos en el modelo Keras\n",
        "    keras_model.set_weights(keras_weights)\n",
        "\n",
        "    return keras_model\n",
        "\n",
        "# Uso con carga de pesos\n",
        "# encoder = create_encoder_cnn(embed_size=256)\n",
        "# encoder = load_pytorch_weights(encoder, 'pytorch_weights.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8482afbb",
      "metadata": {
        "id": "8482afbb"
      },
      "source": [
        "## Lectura y reescritura de código"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f0070f4",
      "metadata": {
        "id": "0f0070f4"
      },
      "source": [
        "### encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fb932b0",
      "metadata": {
        "id": "7fb932b0",
        "outputId": "c763b46d-43c4-46ff-f446-d7715040dd87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m234698864/234698864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import ResNet152\n",
        "from tensorflow.keras.applications.resnet import preprocess_input\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "class EncoderCNN(keras.Model):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "\n",
        "        # Cargar ResNet-152 preentrenado\n",
        "        self.resnet_base = ResNet152(\n",
        "            weights='imagenet',\n",
        "            include_top=False,\n",
        "            input_shape=(224, 224, 3)\n",
        "        )\n",
        "\n",
        "        # Congelar pesos de ResNet\n",
        "        self.resnet_base.trainable = False\n",
        "\n",
        "        # Capas adicionales\n",
        "        self.global_pool = GlobalAveragePooling2D()\n",
        "        self.linear = Dense(embed_size)\n",
        "        self.bn = BatchNormalization(momentum=0.01)\n",
        "\n",
        "    def forward(self, images, training=False):\n",
        "        # Preprocesar imágenes para ResNet\n",
        "        x = preprocess_input(images)\n",
        "\n",
        "        # Pasar por ResNet (siempre en modo inference)\n",
        "        x = self.resnet_base(x, training=False)\n",
        "\n",
        "        # Global Average Pooling\n",
        "        x = self.global_pool(x)\n",
        "\n",
        "        # Capas fully connected\n",
        "        x = self.linear(x)\n",
        "        x = self.bn(x, training=training)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d23167e",
      "metadata": {
        "id": "5d23167e"
      },
      "source": [
        "### decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b449af1",
      "metadata": {
        "id": "0b449af1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Input\n",
        "import numpy as np\n",
        "\n",
        "class DecoderRNN(keras.Model):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
        "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = num_layers\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        # Capas del modelo\n",
        "        self.embed = Embedding(vocab_size, embed_size)\n",
        "        self.lstm = LSTM(\n",
        "                        hidden_size,\n",
        "                        return_sequences=True,\n",
        "                        dropout=0.1,\n",
        "                        recurrent_dropout=0.1,\n",
        "                        kernel_initializer='glorot_uniform',\n",
        "                        recurrent_initializer='orthogonal'\n",
        "                    )\n",
        "  # Opcional: dropout para regularización\n",
        "        self.linear = Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        Forward pass para entrenamiento\n",
        "        inputs: tuple (features, captions, lengths)\n",
        "        \"\"\"\n",
        "        features, captions, lengths = inputs\n",
        "\n",
        "        # Embedding de los captions\n",
        "        embeddings = self.embed(captions)  # (batch_size, seq_len, embed_size)\n",
        "\n",
        "        # Concatenar features con embeddings (como en PyTorch)\n",
        "        features_expanded = tf.expand_dims(features, 1)  # (batch_size, 1, embed_size)\n",
        "        embeddings = tf.concat([features_expanded, embeddings], axis=1)\n",
        "\n",
        "        # Para manejar sequences padding en Keras, usamos mask_zero en Embedding\n",
        "        # y el LSTM manejará automáticamente las máscaras\n",
        "\n",
        "        # LSTM\n",
        "        lstm_output, state_h, state_c = self.lstm(embeddings, training=training)\n",
        "\n",
        "        # Capa lineal de salida\n",
        "        outputs = self.linear(lstm_output)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, data):\n",
        "        \"\"\"\n",
        "        Personalizar el paso de entrenamiento si es necesario\n",
        "        \"\"\"\n",
        "        features, captions, lengths = data\n",
        "        targets = captions[:, 1:]  # Shift para teacher forcing\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass\n",
        "            predictions = self((features, captions, lengths), training=True)\n",
        "\n",
        "            # Calcular pérdida (usando solo las posiciones válidas)\n",
        "            mask = tf.sequence_mask(lengths, maxlen=tf.shape(predictions)[1])\n",
        "            loss = self.compiled_loss(targets, predictions, sample_weight=mask)\n",
        "\n",
        "        # Backward pass\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "        return {'loss': loss}\n",
        "\n",
        "    def sample(self, features, states=None):\n",
        "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
        "        sampled_ids = []\n",
        "\n",
        "        # Estado inicial\n",
        "        if states is None:\n",
        "            states = [tf.zeros((features.shape[0], self.hidden_size))\n",
        "                     for _ in range(2 * self.num_layers)]\n",
        "\n",
        "        # Input inicial son las features\n",
        "        inputs = tf.expand_dims(features, 1)  # (batch_size, 1, embed_size)\n",
        "\n",
        "        for i in range(self.max_seq_length):\n",
        "            # LSTM forward\n",
        "            lstm_output, state_h, state_c = self.lstm(inputs, initial_state=states, training=False)\n",
        "            states = [state_h, state_c]\n",
        "\n",
        "            # Capa lineal\n",
        "            outputs = self.linear(tf.squeeze(lstm_output, 1))  # (batch_size, vocab_size)\n",
        "\n",
        "            # Predicción greedy\n",
        "            predicted = tf.argmax(outputs, axis=1)  # (batch_size,)\n",
        "            sampled_ids.append(predicted)\n",
        "\n",
        "            # Siguiente input\n",
        "            inputs = self.embed(predicted)  # (batch_size, embed_size)\n",
        "            inputs = tf.expand_dims(inputs, 1)  # (batch_size, 1, embed_size)\n",
        "\n",
        "        sampled_ids = tf.stack(sampled_ids, axis=1)  # (batch_size, max_seq_length)\n",
        "        return sampled_ids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c29a5565",
      "metadata": {
        "id": "c29a5565"
      },
      "source": [
        "## Cargar los pesos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95ee88d4",
      "metadata": {
        "id": "95ee88d4"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from build_vocab import Vocabulary\n",
        "with open('pytorchtutorial/tutorials/03-advanced/image_captioning/data/vocab.pkl', 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "\n",
        "encoder = EncoderCNN(embed_size=256)\n",
        "\n",
        "decoder = DecoderRNN(\n",
        "    embed_size=256,\n",
        "    hidden_size=512,\n",
        "    vocab_size=len(vocab),\n",
        "    num_layers=1,\n",
        "    max_seq_length=20\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a13f0101",
      "metadata": {
        "id": "a13f0101"
      },
      "outputs": [],
      "source": [
        "load_pytorch_weights(encoder, \"pytorchtutorial/tutorials/03-advanced/image_captioning/models/encoder-5-3000.pkl\")\n",
        "load_pytorch_weights(decoder, \"pytorchtutorial/tutorials/03-advanced/image_captioning/models/decoder-5-3000.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e6d6339",
      "metadata": {
        "id": "6e6d6339"
      },
      "source": [
        "## Investigación avanzada"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58ff8559",
      "metadata": {
        "id": "58ff8559"
      },
      "source": [
        "### ¿Qué pasa si necesito traducir a otro idioma?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f249618",
      "metadata": {
        "id": "9f249618"
      },
      "source": [
        "**1. Cambio de datos de entrenamiento**  \n",
        "Reemplazarías el archivo fra-eng (francés-inglés) por un archivo jpn-eng (japonés-inglés)  \n",
        "El formato sería el mismo: pares de oraciones paralelas, una en japonés y otra en inglés\n",
        "\n",
        "**2. Preprocesamiento específico para japonés**  \n",
        "Tokenización diferente: El japonés requiere tokenización especial (mecab, janome, etc.) ya que no tiene espacios entre palabras  \n",
        "Caracteres especiales: El japonés usa múltiples sistemas de escritura (kanji, hiragana, katakana)  \n",
        "Vocabulario más grande: Debido a los kanji, el vocabulario puede ser significativamente mayor  \n",
        "\n",
        "**3. Modificaciones en el modelo**  \n",
        "Tamaño de vocabulario: Aumentaría considerablemente para el lado japonés  \n",
        "Dimensionalidad de embeddings: Podría necesitarse mayor dimensión para capturar la complejidad del japonés  \n",
        "Longitud de secuencias: Las secuencias en japonés pueden ser más cortas en términos de tokens pero más complejas  \n",
        "\n",
        "**4. Hiperparámetros ajustados**  \n",
        "Mayor capacidad del modelo para manejar la complejidad lingüística  \n",
        "Posiblemente más épocas de entrenamiento  \n",
        "Regularización adicional para evitar sobreajuste  \n",
        "\n",
        "**5. Evaluación diferente**  \n",
        "Métricas de evaluación específicas para pares de idiomas distantes  \n",
        "Consideración de diferencias estructurales entre japonés e inglés  \n",
        "\n",
        "\n",
        "En esencia: El modelo arquitectónico se mantiene igual, pero todos los componentes relacionados con el procesamiento del lenguaje de entrada (tokenización, vocabulario, embeddings) cambian completamente para adaptarse al japonés."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b6de71c",
      "metadata": {
        "id": "0b6de71c"
      },
      "source": [
        "### ¿Cuáles son algunos métodos avanzados de traducción automática?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dae84284",
      "metadata": {
        "id": "dae84284"
      },
      "source": [
        "Los grandes modelos de lenguaje (LLM, por sus siglas en inglés) han conquistado el mundo.\n",
        "Algunos de estos modelos, como el GPT-4 de OpenAI y el PaLM2 de Google, se han entrenado con conjuntos de datos multilingües y, al menos en teoría, también deberían ser muy capaces de realizar tareas de traducción automática.  \n",
        "\n",
        "Los grandes modelos de lenguaje suelen basarse también en estructuras de transformadores. Sin embargo, en comparación con los modelos de traducción automática neuronal (NMT, por sus siglas en inglés) del apartado anterior, se entrenan con volúmenes de texto mucho más extensos y contienen más parámetros del modelo. Los LLM contienen miles de millones de parámetros frente a los pocos cientos de millones de los modelos de NMT bilingües de una sola tarea. Esto hace que los modelos LLM sean más flexibles e «inteligentes» a la hora de interpretar las instrucciones o los «mensajes» del usuario. Esta nueva tecnología abre numerosas posibilidades en términos de adaptación de los modelos mediante el uso de datos empresariales. Dada la potencia de este planteamiento, prefiero hablar en términos de \"personalización\" en lugar de \"adaptación\"."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c7bc321",
      "metadata": {
        "id": "7c7bc321"
      },
      "source": [
        "### ¿Cómo generar imágenes a partir de texto?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b96e9543",
      "metadata": {
        "id": "b96e9543"
      },
      "source": [
        "**Arquitectura Típica de un Sistema Texto-Imagen**\n",
        "- Encoder de Texto:  \n",
        "Convierte el prompt en representación numérico\n",
        "Modelos como CLIP, BERT, T5\n",
        "\n",
        "- Space de Representación Alineado:  \n",
        "Donde texto e imagen se \"encuentran\" conceptualmente  \n",
        "Aprenden que \"un gato negro\" corresponde a ciertos patrones visuales  \n",
        "\n",
        "- Generador de Imágenes:  \n",
        "Transforma la representación alineada en píxeles  \n",
        "Usa los modelos mencionados arriba (Diffusion, GANs, etc.)  \n",
        "\n",
        "- Refinamiento:  \n",
        "Mejora de resolución (super-resolución)  \n",
        "Ajuste de detalles  \n",
        "Filtrado de contenido inapropiado\n",
        "\n",
        "**Modelos Principales**  \n",
        "1. Diffusion Models (Modelos de Difusión)  \n",
        "Proceso de \"ruido\": Comienzan con una imagen de puro ruido aleatorio  \n",
        "Desruido guiado: Eliminan progresivamente el ruido en múltiples pasos  \n",
        "Guía textual: En cada paso, el texto dirige cómo debe \"evolucionar\" la imagen  \n",
        "Analogía: Como esculpir una estatua quitando material sobrante según un plano (el texto)\n",
        "\n",
        "2. Generative Adversarial Networks (GANs)  \n",
        "Dos redes neuronales compitiendo:  \n",
        "Generador: Crea imágenes a partir de texto  \n",
        "Discriminador: Juzga si las imágenes son reales o generadas  \n",
        "Entrenamiento adversarial: Mejora continua mediante competición  \n",
        "Resultado: El generador se vuelve experto en engañar al discriminador  \n",
        "\n",
        "3. Autoregressive Models  \n",
        "Tokenización de imagen: Tratan la imagen como una secuencia de \"tokens\" visuales  \n",
        "Predicción secuencial: Generan la imagen token por token (como escribir una frase)  \n",
        "Condicionamiento textual: Cada token se predice basándose en el texto y tokens anteriores"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}